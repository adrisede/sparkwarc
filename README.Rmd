---
title: "sparkwarc - WARC files in sparklyr"
output:
  github_document:
    fig_width: 9
    fig_height: 5
---

# Install

Install [sparkwarc from CRAN](https://cran.r-project.org/package=sparkwarc) or the dev version with:

```{r eval=FALSE}
devtools::install_github("javierluraschi/sparkwarc")
```

# Intro

The following example loads a very small subset of a WARC file from [Common Crawl](http://commoncrawl.org), a nonprofit 501 organization that crawls the web and freely provides its archives and datasets to the public.

```{r}
library(sparkwarc)
library(sparklyr)
library(DBI)
library(dplyr)
```

```{r max.print=10}
sc <- spark_connect(master = "local", version = "2.0.1")
spark_read_warc(
  sc,
  "warc",
  system.file("samples/sample.warc.gz", package = "sparkwarc"),
  repartition = 8)
```

```{sql connection=sc, max.print=1}
SELECT count(value)
FROM WARC
WHERE length(regexp_extract(value, '<html', 0)) > 0
```

```{r}
cc_stats <- function(ops, value) {
  ops %>%
    filter(value != "") %>%
    group_by(value) %>%
    summarize(count = n()) %>%
    arrange(desc(count)) %>%
    head(100)
}

cc_regex <- function(regex) {
  tbl(sc, "warc") %>%
    transmute(regval = regexp_extract(value, regval, 1)) %>%
    cc_stats(regex)
}
```

```{r}
cc_stats("http-equiv=\"Content-Language\" content=\"(.*)\"")
```

```{r}
cc_stats("<script .*src=\\\".*/(.+)\\\".*")
```

```{r}
cc_stats("<([a-zA-Z]+)>")
```

```{r}
cc_stats(" ([a-zA-Z]{5,10}) ")
```

```{r}
cc_stats("<meta .*keywords.*content=\"([^,\\\"]+).*")
```

```{r}
cc_stats("<script .*src=\\\".*/([^/]+.js)\\\".*")
```

```{r}
spark_disconnect(sc)
```

# Querying 1GB

```{r}
config <- spark_config()
config[["spark.memory.fraction"]] <- "0.9"
config[["spark.executor.memory"]] <- "10G"
config[["sparklyr.shell.driver-memory"]] <- "10G"

sc <- spark_connect(master = "local", version = "2.0.1", config = config)
spark_read_warc(
  sc,
  "warc",
  "/Users/javierluraschi/Downloads/CC-MAIN-20161202170900-00000-ip-10-31-129-80.ec2.internal.warc",
  repartition = 8)
```

df <- data.frame(list(a = list("a,b,c")))

```{sql connection=sc, max.print=1}
SELECT count(value)
FROM WARC
WHERE length(regexp_extract(value, '<([a-z]+)>', 0)) > 0
```

```{sql connection=sc, max.print=1}
SELECT count(value)
FROM WARC
WHERE length(regexp_extract(value, '<html', 0)) > 0
```

```{r}
cc_stats("http-equiv=\"Content-Language\" content=\"([^\\\"]*)\"")
```

```{r}
cc_stats("WARC-Target-URI: http://([^/]+)/.*")
```

```{r}
cc_stats("<([a-zA-Z]+)>")
```

```{r}
cc_stats("<meta .*keywords.*content=\"([a-zA-Z0-9]+).*")
```

# Querying 1PB

By [running sparklyr in EMR](https://aws.amazon.com/blogs/big-data/running-sparklyr-rstudios-r-interface-to-spark-on-amazon-emr/), one can configure an EMR cluster and load about **~5GB** of data using:

```{r eval=FALSE}
sc <- spark_connect(master = "yarn-client")
spark_read_warc(sc, "warc", cc_warc(1, 1))

tbl(sc, "warc") %>% summarize(n = n())
spark_disconnect_all()
```

To read the first 200 files, or about **~1TB** of data, first scale the cluster, consider maximizing resource allocation with the followin EMR config:

```
[
  {
    "Classification": "spark",
    "Properties": {
      "maximizeResourceAllocation": "true"
    }
  }
]
```

Followed by loading the `[1, 200]` file range with:

```{r eval=FALSE}
sc <- spark_connect(master = "yarn-client")
spark_read_warc(sc, "warc", cc_warc(1, 200))

tbl(sc, "warc") %>% summarize(n = n())
spark_disconnect_all()
```

To read the entire crawl, about **~1PB**, a custom script would be needed to load all the WARC files.
